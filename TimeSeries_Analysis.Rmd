---
title: "TimeSeries_Analysis"
author: "Yitong Chen"
date: "10/16/2020"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE}
library(ggplot2)
library(forecast)
library(fpp)
library(gridExtra)
library(astsa)
library(forecast)
library(TSA)
library(cowplot)
library(dplyr)
```

### Introduction: 
This Data Set contains 180 data points with column one representing time (in months) and column two representing some unknown observations. <br/>

load the dataset
```{r}
data1 <- read.csv("./data1.csv")
```

### 1 Exploratory Data Analysis
#### 1.1 Stationarity & Variance
Let's define the our initial Time Series as $A_{t}$. <br/><br/>
We began our EDA by first observing its general trend and shape (Figure 1.1a). Through its obvious upward trend, heteroscedasticity, and potential seasonality, we understood that it was clearly not stationary time series. Thus, to address the issue, we began our journey to make predictions by conducting a variance stabilizing transformation followed by differencing in order to achieve stationary data for the purpose of using an appropriate (M)(S)ARIMA model.<br/><br/>

To remove the unstationarities, we do the following transformations: <br/>
1) Take Logarithm        ($A_{t} -> L_{t}$)    : $L_{t} = log{A_t}$ <br/>
2) Take First Difference ($L_{t} -> X_{t}$)    : $X_{t} = L_{t} - L_{t-1}$
```{r, warning=FALSE}
# figure 1.1a
plot1 <- ggplot() +
  geom_line(data = data1, aes(X, x), color = "black")  + labs(title="Plot of Orginal Time Series, At",
        x ="Time", y = "X")

# figure 1.1b  take logarithm
data1 <- data1 %>% mutate(Lt = log(data1$x))
plot2 <- ggplot() +
  geom_line(data = data1, aes(X, Lt), color = "black")  + labs(title="Plot of Log Transformed Data, Lt",
        x ="Time", y = "Nt")

# figure 1.1c  take 1st difference
data1 <- data1 %>% mutate(Xt = c(NA, diff(data1$x, lag = 1)))
plot3 <- ggplot() +
  geom_line(data = data1, aes(X, Xt), color = "black") + geom_hline(yintercept = 0, 
                color = "blue", size=0.7) + labs(title="Plot of 1st Difference Time Series, Xt",
        x ="Time", y = "Xt")

grid.arrange(plot1, plot2, plot3, ncol = 2)
```


1.2 ACF & PACF of Xt
Looking at Figure 1.2a, we observe that the ACF and PACF of $X_{t}$ are significant at around lag 6, 12, 18, 24, a.k.a implying a seasonal pattern of 6 periods. This is suggesting us to remove the seasonalities by differencing.

1.3 Seasonal differencing<br/>
Take Sixth Difference on $X_{t}$ ($X_{t} -> Y_{t}$)    : $Y_{t} = X_{t} - X_{t-12}$
```{r, warning=FALSE}
p1 <- ggAcf(data1$Xt, lag.max = 120)+ ggtitle("Xt ACF")
p2 <- ggPacf(data1$Xt, lag.max = 120) + ggtitle("Xt PACF")
#grid.arrange(p1, p2, nrow = 2)

data1 <- data1 %>% mutate(Yt = c(NA, NA,NA,NA,NA,NA,NA, NA,NA,NA,NA,NA, diff(data1$Xt, lag = 12)))
plot4 <- ggplot() +
  geom_line(data = data1, aes(X, Yt), color = "black") + geom_hline(yintercept = 0, 
                color = "blue", size=0.7) + labs(title="Plot of Yt, a.k.a Xt with Seasonality Removed",
        x ="Time", y = "Yt")

grid.arrange(p1, p2,plot4, nrow = 2)
```


```{r, warning=FALSE}
adf.test(na.remove(data1$Yt))
```


1.4 select a model<br/>
Augmented Dicky-Fuller test indicates if the data correponds to a stationary process, and the above test shows $Y_{t}$ is significant at 0.01 confidence level. Therefore, we are quite convinced that $Y_{t}$ is a stationary process. We'll fit an ARMA model from here.<br/>
From PACF, we see significant lags at lag4, lag5, lag7, lag9, lag14; from the ACF, we still see a seasonal pattern at period of 12. Since a seansonal differencing has applied here, it's reasonable to propose a GARCH model here(we'll discuss this in Second Section). But we'll stick to SARIMA within this section. 
```{r}
p3 <- ggAcf(data1$Yt, lag.max = 60)+ ggtitle("Yt ACF")
p4 <- ggPacf(data1$Yt, lag.max = 60) + ggtitle("Yt PACF")
grid.arrange(p3, p4, nrow = 2)
```

1.5 Demo Fit an ARMA model on $Y_{t}$ 
Based on the ACF and PACF, we'll fit ARMA data on $Y_{t}$, A.K.A a $SARIMA(p,1,q)*(P,1,Q)_{12}$ Model on the log transformed of original data($L_{t}$). Since there's not an explicit obvious ARMA pattern given in $Y_{t}$, a few options are proposed here and we will later on make a model selection based on the Cross Validation results.<br/>
Option1 : ARMA(4,1) ||| Reason: We have two very relevant AR factor ar1 ar4, as well as an MA factor ma1
```{r, warning=FALSE}
## Model 1 summary
model1 <- arma(na.remove(data1$Yt), order = c(4,2), include.intercept = FALSE)
summary(model1)

## plot the Yt_hat and Yt
model1_perf <- data.frame(c(1:180), c(NA, NA,NA,NA,NA,NA,NA,NA, NA,NA,NA,NA,NA, model1$fitted.values))
x <- c("X", "Yt_fitted")
colnames(model1_perf) <- x

plot5 <- ggplot() +
  geom_line(data = model1_perf, aes(X, Yt_fitted), color = "black") +
  geom_line(data = data1, aes(X, Yt), color = "red") + ggtitle("model1, Yt red, Yt_fitted black")+
  geom_hline(yintercept = 0, color = "blue", size=0.7) 
  
plot5

```


1.6 From here we'll officially comparing a few potential models, to avoid the redundant procedures above, we won't repeat the above process for tidyness, but they are essentially similar. Instead, to give better interpretation, we'll leverage the SARIMA and SARIMA.for functions for our model validation process. For consistency purpose, we would also apply the SARIMA function on model1 again.<br/><br/>

Model1: $SARIMA(4,1,2)(0,1,0)_{12}$
```{r}
## Predictions
sarima.for(data1$Lt, p=4, d=1, q=2, P=0, D=1, Q=0, S = 12,n.ahead = 12)
```

```{r}
## validation

## a helper function to avoid sarima functions generating tons of graph during validation
sarima.noplot = function(x0, x1, x2, x3, x4, x5, x6, x7, x8) {
  png(tf<-tempfile())
  out <- sarima.for(x0, p=x1, d=x2, q=x3, P=x4, D=x5, Q=x6, S = x7,n.ahead = x8)
  dev.off()
  file.remove(tf)
  return(out)
}
```

```{r}
pred_value = matrix(0, nrow = 180, ncol = 1)

i = 1
while(i <= 156){
  data1sub = data1$Lt[data1$X < i + 24]
  # model1
  cvmodel1 <- sarima.noplot(x0 =data1sub, x1=4, x2=1, x3=2, x4=0, x5=1, x6=0, x7 = 12, x8= 12)
  pred_value[(24+i):(35+i),] <- exp(cvmodel1$pred)
  i = i + 12
}

## plot the cross validation result against the true value
pred_value <- data.frame(c(1:180),pred_value)
ggplot() + 
  geom_line(data = data1, aes(X, x), color = "black")  +labs(title="Plot of At(dark) versas At_hat(red), by Model1 SARIMA(4,1,2)(0,1,0)12", x ="Time", y = "X") + 
  geom_line(data = pred_value, aes(c.1.180., pred_value), color = "red")
```


Model2: $SARIMA(1,1,0)(0,1,1)_{6}$
```{r}
## Predictions
sarima.for(data1$Lt, p=1, d=1, q=0, P=0, D=1, Q=1, S = 12,n.ahead = 12)
```

```{r}
pred_value = matrix(0, nrow = 180, ncol = 1)

i = 1
while(i <= 156){
  data1sub = data1$Lt[data1$X < i + 24]
  # model1
  cvmodel1 <- sarima.noplot(x0 =data1sub, x1=1, x2=1, x3=0, x4=0, x5=1, x6=1, x7 = 12, x8= 12)
  pred_value[(24+i):(35+i),] <- exp(cvmodel1$pred)
  i = i + 12
}

## plot the cross validation result against the true value
pred_value <- data.frame(c(1:180),pred_value)
ggplot() + 
  geom_line(data = data1, aes(X, x), color = "black")  +labs(title="Plot of At(dark) versas At_hat(red), by Model1 SARIMA(1,1,0)(0,1,1)12", x ="Time", y = "X") + 
  geom_line(data = pred_value, aes(c.1.180., pred_value), color = "red")
```



## 1.7 Discussion on Model selection
```{r, warning=FALSE, results = "hide"}
model1 <- sarima(data1$Lt, p=4, d=1, q=2, P=0, D=1, Q=0, S = 12)
model2 <- sarima(data1$Lt, p=1, d=1, q=0, P=0, D=1, Q=1, S = 12)
```


```{r}
ICs <-sapply(list(model1, model2),function(mdl) {c(mdl$AIC, mdl$AICc, mdl$BIC)})
colnames(ICs) <-paste0("Model", 1:2)
rownames(ICs) <-c('AIC','AICc','BIC')
ICs
```



